{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"problema\"></a>\n",
    "# <font color=green>Problem statement</font>\n",
    "\n",
    "For every aspiring entrepreneur, embarking on the journey of starting a new business presents a myriad of challenges. The most daunting of these challenges is the uncertainty surrounding access to vital information that supports informed decision-making and helps mitigate the risks associated with investing time and money.\n",
    "\n",
    "In light of this, providing entrepreneurs with tools that furnish organized, accurate, and reliable information would be immensely beneficial. Such tools would offer them a sense of security and heightened confidence in assessing the profitability of their business ideas.\n",
    "\n",
    "Henceforth, unquestionably, **data science** emerges as the optimal tool for crafting such support tools for entrepreneurs and their nascent businesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preguntas\"></a>\n",
    "# <font color=green>Asking Questions </font>\n",
    "\n",
    "In accordance with the problem posed above, the following questions arose from both the entrepreneur and the team:\n",
    "\n",
    "1. What is the best location in Guadalajara, Mexico to open my bicycle business?\n",
    "2. How many sales will I be able to obtain in the first months of starting my business?\n",
    "3. What are the products most in demand by potential customers?\n",
    "4. What prices will be the most competitive for bicycles?\n",
    "5. How often will a customer want to buy clothing or accessories for their bikes?\n",
    "6. How many customers will come to my business for a repair or upgrade on their bicycle?\n",
    "\n",
    "While we cannot guarantee that we will address all the questions raised, we assured the entrepreneur that we will conduct an analysis of the available data. Subsequently, we will evaluate which information can be presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "The decision was made to use \"Bike Buyers 1000\" and \"Bike Sales\", given that they contain information relevant to the problem being discussed and both datasets complement each other, which were found on kaggle.\n",
    "\n",
    "\"Bike Buyers 1000\" \n",
    "link: \"https://www.kaggle.com/datasets/heeraldedhia/bike-buyers\"\n",
    "\n",
    "\"Bike Sales\"\n",
    "link: \"https://www.kaggle.com/datasets/liyingiris90/bike-sales\"\n",
    "\n",
    "## Process to obtain the dataset\n",
    "\n",
    "Since the files contained within the Bike Sales data set are in .xlsx format, the Pandas function used was <font color =red> *pd.read_excel* </font>.\n",
    "\n",
    "For Bike Buyers, since it is a csv format, our already known function is used  <font color =red> *pd.read_csv*</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from colorama import Fore\n",
    "from colorama import Style\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "from scipy.stats import skew, kurtosis\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The respective DataFrames of the data set are saved <font color=blue> \"bike sales\"</font> and <font color=blue> \"bike buyers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers = pd.read_csv(\"datasets/bike_buyers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = pd.read_excel(\"datasets/bikes.xlsx\")\n",
    "bikestores = pd.read_excel(\"datasets/bikeshops.xlsx\")\n",
    "orders = pd.read_excel(\"datasets/orders.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "We proceed to review each DataFrame using <font color=blue> .head() </font> and <font color=blue>.tail()  </font>, as well as <font color=blue> .dtypes, .columns, .shape, .loc[$n:m$] </font> (where $n,m ‚àà ùñ≠$ and $n<m.$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers.dtypes # There are floats that should be integers.\n",
    "buyers.shape # There are 1000 entries and 13 columns.\n",
    "buyers.columns # ID, Marital S, Gender, Income, Children, Education, Occupation, Home Owner, Cars, Commute Distance, Region, Age, purchase bike\n",
    "buyers.head() # NaNs are observed\n",
    "buyers.tail() # NaNs are observed\n",
    "c=list(buyers.columns) # The name of the columns is saved in a list, which will be used later to change the names of our dataframe. \n",
    "buyers.loc[500:515] # NaNs are observed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.dtypes # The only data that appears to be incorrect is the price, which is of type int. \n",
    "bikes.shape # There are a total of 97 rows with 5 columns\n",
    "bikes.columns # bike.id, model, category1, category2, frame, price. Columns will be renamed.\n",
    "bikes.head() # NaNs are not observed.\n",
    "bikes.tail() # NaNs are not observed.\n",
    "bc=list(bikes.columns)\n",
    "bikes.loc[46:58] # NaNs are not observed randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikestores.dtypes # All data has the correct type.\n",
    "bikestores.shape # There are a total of 30 rows with 6 columns\n",
    "bikestores.columns # bikeshop.id , bikeshop.name, bikeshop.city, bikeshop.state, latitude , longitude. Columns will be renamed.\n",
    "bikestores.head() # NaNs are not observed.\n",
    "bikestores.tail() # NaNs are not observed.\n",
    "bs=list(bikestores.columns)\n",
    "bikestores.loc[10:25] # NaNs are not observed randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.dtypes # It is observed that order.id, order.line, customer.id y product.id They are of type float when they should be of type integer.\n",
    "orders.shape # There are a total of 15644 rows and 7 columns. \n",
    "orders.columns # 'Unnamed: 0', 'order.id', 'order.line', 'order.date', 'customer.id','product.id' y 'quantity'. Columns will be renamed.\n",
    "oc=list(orders.columns) \n",
    "orders.head() # No Nans are observed, on the other hand, it is observed that the column 'Unnamed: 0' is repeated. \n",
    "orders.tail() # NaNs are not observed. \n",
    "orders.loc[10468:10480] # NaNs are not observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the values of some columns of some dataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['product.id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['customer.id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['bike.id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikestores['bikeshop.id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename the columns in the DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries are created to change the name of the columns of our data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_names = {c[0]:'id_buyer',\n",
    "              c[1]:'civil_status',\n",
    "              c[2]:'gender',\n",
    "              c[3]:'salary',\n",
    "              c[4]:'children',\n",
    "              c[5]:'education',\n",
    "              c[6]:\"profession\",\n",
    "              c[7]:\"own_house\",\n",
    "              c[8]:'cars',\n",
    "              c[9]:'trip_distance',\n",
    "              c[10]:'region',\n",
    "              c[11]:'age',\n",
    "              c[12]:'purchased_bicycle'}\n",
    "\n",
    "bike_names = {bc[0]:'id_bicycle',\n",
    "              bc[1]:'model',\n",
    "              bc[2]:'category_1',\n",
    "              bc[3]:'category_2',\n",
    "              bc[4]:'alloy',\n",
    "              bc[5]:'price'}\n",
    "\n",
    "stores_names = {bs[0]:'id_store',\n",
    "                  bs[1]:'store_name',\n",
    "                  bs[2]:'city',\n",
    "                  bs[3]:'state',\n",
    "                  bs[4]:'latitude',\n",
    "                  bs[5]:'length'}\n",
    "\n",
    "orders_names = {oc[1]:'id_order',\n",
    "                  oc[2]:'order_line',\n",
    "                  oc[3]:'order_date',\n",
    "                  oc[4]:'id_store',\n",
    "                  oc[5]:'id_bicycle',\n",
    "                  oc[6]:'items_number'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice the fact, that the customer_id column is actually the store_id. Since both match the number of entries and the values associated with each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers = buyers.rename(columns=buy_names)\n",
    "orders = orders.rename(columns=orders_names)\n",
    "bikes = bikes.rename(columns=bike_names)\n",
    "stores =  bikestores.rename(columns=stores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers.name = 'buyers'\n",
    "orders.name = 'orders'\n",
    "bikes.name = 'bikes'\n",
    "stores.name = 'stores'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of NaNs\n",
    "\n",
    "To have cleaner DataFrames, the NaNs are eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions are created to determine NaNs between the different DataFrames.\n",
    "def nans_numbers(dataframe):\n",
    "    print(dataframe.isna().sum())\n",
    "\n",
    "def nans_percentage(dataframe):\n",
    "    print(dataframe.name)\n",
    "    print(dataframe.isna().sum()/len(dataframe)*100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_percentage(orders)\n",
    "nans_percentage(stores)\n",
    "nans_percentage(bikes)\n",
    "nans_percentage(buyers) # The only DataFrame with NaNs present is the Buyers one.. \n",
    "nans_numbers(buyers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers[['children','cars']] = buyers[['children','cars']].fillna(0)\n",
    "buyers['civil_status'] = buyers['civil_status'].fillna('Single')\n",
    "buyers['own_house'] = buyers['own_house'].fillna('No')\n",
    "buyers =buyers.dropna(how='any')\n",
    "buyers = buyers.reset_index(drop=True) \n",
    "nans_numbers(buyers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We proceed to change the data type of each DataFrame\n",
    "\n",
    "For this we will create dictionaries with the new data types and apply the pandas function *.astype()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orders.name)\n",
    "print(orders.dtypes) # It has floats that must be integers. \n",
    "print(\"\\n\")\n",
    "print(stores.name)\n",
    "print(stores.dtypes) # Stores has all the correct data types.\n",
    "print(\"\\n\")\n",
    "print(buyers.name)\n",
    "print(buyers.dtypes) # It has floats that must be integers. \n",
    "print(\"\\n\")\n",
    "print(bikes.name)\n",
    "print(bikes.dtypes) # It has an int that should be float.  \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=list(buyers.columns)\n",
    "oc=list(orders.columns)\n",
    "new_types_buy = {c[4]:int,\n",
    "         c[8]:int,\n",
    "         c[11]:int}\n",
    "\n",
    "new_types_orders ={oc[1]:int,\n",
    "                   oc[2]:int,\n",
    "                   oc[4]:int,\n",
    "                   oc[5]:int,\n",
    "                   oc[6]:int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = orders.astype(new_types_orders)\n",
    "buyers = buyers.astype(new_types_buy)\n",
    "bikes['price'] = bikes['price'].astype(float) # This method is used, since you only have to change the data type for one variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will proceed to eliminate the column 'Unnamed: 0' from the DataFrame orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.drop(columns='Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We proceed to join the different tables to later make the appropriate aggregations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orders and Bikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_bikes = pd.merge(orders,bikes,left_on='id_bicycle',right_index=True)\n",
    "or_bikes.drop(columns=['id_bicycle_x','id_bicycle_y'], inplace=True)\n",
    "or_bikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stores and Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_orders = pd.merge(orders,stores,left_on='id_store',right_index=True)\n",
    "str_orders.drop(columns=['id_store_x','id_store_y'])\n",
    "str_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stores, Bikes and Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the true earnings for each bike, a new column called total is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sell = or_bikes['items_number']*or_bikes['price']\n",
    "or_bikes['total'] = total_sell\n",
    "or_bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_or_bks = pd.merge(or_bikes,stores,left_on='id_store',right_index=True)\n",
    "str_or_bks.drop(columns=['id_store_x','id_store_y'],inplace=True)\n",
    "str_or_bks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of items sold by each store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_orders.groupby(['store_name'])['items_number'].sum().sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stores that have sold the most with their respective total are...\n",
    "\n",
    ">New York Cycles          ~       3471\n",
    "\n",
    ">Minneapolis Bike Shop      ~     2301\n",
    "\n",
    ">Las Vegas Cycles            ~    1419\n",
    "\n",
    ">Columbus Race Equipment      ~   1264\n",
    "\n",
    ">Albuquerque Cycles            ~  1155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While those with smaller sales are made up of... with a total of...\n",
    "\n",
    ">Phoenix Bi-peds        ~          246\n",
    "\n",
    ">Providence Bi-peds      ~       245\n",
    "\n",
    ">Oklahoma City Race Equipment  ~   234\n",
    "\n",
    ">Wichita Speed                  ~ 186\n",
    "\n",
    ">Ann Arbor Speed                ~  128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Less and Most Sold Bicycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_bikes.groupby('model')['items_number'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-selling Bicycles were... with a total of...\n",
    "\n",
    ">Slice Ultegra        ->              301\n",
    "\n",
    ">F-Si 3                ->             293\n",
    "\n",
    ">F-Si Black Inc.         ->           293\n",
    "\n",
    ">Supersix Evo Ultegra 4    ->         290\n",
    "\n",
    ">CAAD Disc Ultegra           ->       282\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the least sold were... with a total of...\n",
    "\n",
    ">Habit 4              ~              143\n",
    "\n",
    ">Habit Carbon 2        ~             143\n",
    "\n",
    ">Habit Carbon SE        ~            140\n",
    "\n",
    ">Synapse Carbon Disc Ultegra D12  ~  139\n",
    "\n",
    ">Jekyll Carbon 4                ~    132"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sells obtained by each Bicycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_bikes.groupby('model')['total'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best and worst sales obtained per bicycle were the following... with a total of...\n",
    "\n",
    ">Scalpel-Si Black Inc.     ~        3299820.0\n",
    "\n",
    ">F-Si Black Inc.            ~       3278670.0\n",
    "\n",
    ">Habit Hi-Mod Black Inc.     ~      2670500.0\n",
    "\n",
    ">Trigger Carbon 1             ~     2140200.0\n",
    "\n",
    ">Synapse Hi-Mod Disc Black Inc. ~    2100210.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the worst...\n",
    "\n",
    ">Trail 5    ~                        212715.0\n",
    "\n",
    ">Catalyst 1  ~                      184710.0\n",
    "\n",
    ">Catalyst 2   ~                      119925.0\n",
    "\n",
    ">Catalyst 3    ~                     118080.0\n",
    "\n",
    ">Catalyst 4     ~                     96695.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales obtained by each Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_or_bks.groupby('store_name')['total'].sum().sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see that the stores with the highest sales are... with a total of...\n",
    "\n",
    ">New York Cycles         ~        12476315.0\n",
    "\n",
    ">Minneapolis Bike Shop    ~        8018850.0\n",
    "\n",
    ">Columbus Race Equipment   ~       5635205.0\n",
    "\n",
    ">Las Vegas Cycles           ~      4763175.0\n",
    "\n",
    ">Albuquerque Cycles          ~     3959000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While those with lower profits...\n",
    "\n",
    ">Providence Bi-peds                819110.0\n",
    "\n",
    ">Oklahoma City Race Equipment ~     763390.0\n",
    "\n",
    ">Phoenix Bi-peds               ~    737905.0\n",
    "\n",
    ">Wichita Speed                  ~   665510.0\n",
    "\n",
    ">Ann Arbor Speed                 ~  408680.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales per order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_bikes.groupby('id_order')['total'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best cities to sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_or_bks.groupby('city')['total'].sum().sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see below, the best cities are... with a total raised of...\n",
    "\n",
    ">*New York*   ~      12476315.0\n",
    "\n",
    ">Minneapolis    ~   8018850.0\n",
    "\n",
    ">Columbus        ~  5635205.0\n",
    "\n",
    ">Las Vegas        ~ 4763175.0\n",
    "\n",
    ">Albuquerque       ~ 3959000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cities with fewer sales\n",
    "\n",
    ">Providence   ~      819110.0\n",
    "\n",
    ">Oklahoma City ~     763390.0\n",
    "\n",
    ">Phoenix        ~    737905.0\n",
    "\n",
    ">Wichita         ~   665510.0\n",
    "\n",
    ">Ann Arbor        ~  408680.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First months of sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Sales per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_bikes.groupby([pd.Grouper(key='order_date', axis=0, \n",
    "                      freq='ME')])['total'].sum().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_bikes.groupby([pd.Grouper(key='order_date', axis=0, freq='ME')])['total'].mean().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total sales for each month of each store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_bikes.groupby(['id_store',pd.Grouper(key='order_date', axis=0, \n",
    "                      freq='ME')])['total'].agg('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alloy and best-selling categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_bikes.groupby(['alloy','category_1','category_2'])['total'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Average, Median, Truncated Mean, Standard Deviation, Range and Interquartile, 25th and 75th Percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data to be worked on, there are 13 aggregations so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_buyers = buyers[buyers['purchased_bicycle']=='Yes'].reset_index(drop=True)\n",
    "real_buyers.drop(columns='purchased_bicycle',inplace=True) # To omit redundancy, the 'purchased_bicycle' column is eliminated\n",
    "real_buyers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is decided to obtain estimates of location and variability in the number of items sold by each store, bicycle, profits obtained, and age of bicycle buyers. This is in order to be able to observe whether there are atypical values in our data or not, and which may present a problem when evaluating our regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_sold_per_tienda = str_orders.groupby(['store_name'])['items_number'].sum().sort_values(ascending=False)\n",
    "items_sold_per_bici = or_bikes.groupby('model')['items_number'].sum().sort_values(ascending=False)\n",
    "gains_per_bici = or_bikes.groupby('model')['total'].sum().sort_values(ascending=False)\n",
    "gains_per_tienda = str_or_bks.groupby('store_name')['total'].sum().sort_values(ascending=False)\n",
    "gain_per_order = or_bikes.groupby('id_order')['total'].sum().sort_values(ascending=False)\n",
    "gain_per_city = str_or_bks.groupby('city')['total'].sum().sort_values(ascending=False)\n",
    "gain_per_month_from_all = or_bikes.groupby([pd.Grouper(key='order_date', axis=0, freq='ME')])['total'].sum()\n",
    "gain_per_month_per_tienda = or_bikes.groupby(['id_store',pd.Grouper(key='order_date', axis=0, freq='ME')])['total'].sum();\n",
    "or_bikes['price']\n",
    "or_bikes['total'];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Location and Variability Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_in_one_variability(x):\n",
    "  print(f'{Fore.CYAN}Mean {Style.RESET_ALL}~ {x.mean()}') \n",
    "  print(f'{Fore.CYAN}Median {Style.RESET_ALL}~ {x.median()}') \n",
    "  print(f'{Fore.CYAN}Truncated Mean {Style.RESET_ALL}~ {stats.trim_mean(x,.1)}') # Truncated mean of 10% of the data on the sides.\n",
    "  print(f'{Fore.CYAN}Standard Deviation {Style.RESET_ALL}~ {x.std()}')    \n",
    "  print(f'{Fore.CYAN}Range {Style.RESET_ALL}~ {x.max() - x.min()}')\n",
    "  print(f'{Fore.CYAN}Quantil 75 {Style.RESET_ALL}~ {x.quantile(.75)}') \n",
    "  print(f'{Fore.CYAN}Quantil 25 {Style.RESET_ALL}~ {x.quantile(.25)}') \n",
    "  print(f'{Fore.CYAN}Interquartile Range {Style.RESET_ALL}~ {x.quantile(.75) - x.quantile(.25)}') # Interquartile range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(items_sold_per_tienda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(items_sold_per_bici)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(gains_per_bici)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(gains_per_tienda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(gain_per_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(gain_per_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(gain_per_month_from_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(gain_per_month_per_tienda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(or_bikes['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(real_buyers['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Salary (Does not imply that they have purchased a bicycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(buyers['salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bicycle Buyers Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one_variability(real_buyers['salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "> Since there is too much data to analyze, a conclusion is left at the end of all our graphs and general findings at the end. However, an analysis is carried out on the data that is considered most relevant for our future models..\n",
    "\n",
    "> Raw ~ The data that has not undergone changes, such as the Interquartile Range Score (IQR-Score), is used as a reference.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette(\"crest\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Density, Histogram and Boxplot Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphics_boxplots_density(df,i):\n",
    "  o=['Items sells per store','Demand for bikes','Profits per bike','Earnings per store','Earnings per order','Earnings per city', 'General sales per month','Sales per month of each store','SubTotal','Customer age','Age of buyers'\n",
    "  ,'Client Salary','Buyers salary']\n",
    "  sns.set_theme(style=\"white\")\n",
    "  fig, axes = plt.subplots(1, 3, figsize=(30, 6), sharex=False, sharey=False)\n",
    "  sns.boxplot(x=df, ax=axes[0]); # Unfiltered Boxplot\n",
    "  axes[0].set_title(o[i], fontsize=20)\n",
    "  axes[0].set_xlabel('',fontsize = 16)\n",
    "  iqr = df.quantile(0.75) - df.quantile(0.25)\n",
    "  bottom_filter = df > df.quantile(0.25) - (iqr * 1.5)\n",
    "  top_filter = df < df.quantile(0.75) + (iqr * 1.5)\n",
    "  df_filtrado = df[bottom_filter & top_filter]\n",
    "  c = ' ~ Filtered out ~ IQR'\n",
    "  b = o[i] + c\n",
    "  axes[1].set_title(b,fontsize=20)\n",
    "  axes[1].set_xlabel('' ,fontsize = 16)\n",
    "  sns.boxplot(df_filtrado,ax=axes[1])\n",
    "  d = ' ~ Density and Histogram'\n",
    "  e = o[i] + d\n",
    "  sns.displot(df_filtrado, bins=16);\n",
    "  plt.ylabel('Frequency', fontsize=16);\n",
    "  print(f'{Fore.MAGENTA}Location and Variability Estimates ~ Unfiltered data{Style.RESET_ALL}')\n",
    "  all_in_one_variability(df)\n",
    "  print(f'{Fore.GREEN}Kurtosis:{Style.RESET_ALL} {kurtosis(df)}')\n",
    "  print(f'{Fore.GREEN}Asymmetry: {Style.RESET_ALL}{skew(df)}')\n",
    "  print(f'{Fore.MAGENTA}Location and Variability Estimates ~ Filtered data ~ Range IQR{Style.RESET_ALL}')\n",
    "  all_in_one_variability(df_filtrado)#NorbsPR did this.\n",
    "  print(f'{Fore.GREEN}Kurtosis:{Style.RESET_ALL} {kurtosis(df_filtrado)}')\n",
    "  print(f'{Fore.GREEN}Asymmetry:{Style.RESET_ALL} {skew(df_filtrado)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphics_boxplots_density(items_sold_per_tienda,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demand for bicycles (Number of bicycles sold by model)\n",
    "\n",
    "> On this occasion it is observed that the values do not change after performing the Interquantile Range Score. Therefore, we will work with the original data.\n",
    "\n",
    "> We can see in the boxplot that the data accumulates a little towards the right tail. But not significantly.\n",
    "\n",
    "> After looking at the asymmetry value (0.05) it is suggested that perhaps the distribution is normal, but after seeing the histogram/density graph we realize that our distribution is actually bimodal.\n",
    "\n",
    "> On the other hand, the kurtosis is -1.31, which indicates that the data is steep, so there is less dispersion of the data and therefore short tails.\n",
    "> Finally we have a standard deviation of 47.30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
